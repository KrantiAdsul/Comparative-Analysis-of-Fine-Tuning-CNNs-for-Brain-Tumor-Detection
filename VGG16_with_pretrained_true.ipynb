{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba31b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87747ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba490b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Baseline VGG16 Model using pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/Kranti/Desktop/EE541_project/archive_new/Training'\n",
    "val_dir = 'C:/Users/Kranti/Desktop/EE541_project/archive_new/Validation'\n",
    "test_dir = 'C:/Users/Kranti/Desktop/EE541_project/archive_new/Testing/testing'\n",
    "\n",
    "# Print the number of images in each category for the training set\n",
    "print('Training set:')\n",
    "for class_name in os.listdir(train_dir):\n",
    "    class_dir = os.path.join(train_dir, class_name)\n",
    "    print(class_name + ':', len(os.listdir(class_dir)))\n",
    "\n",
    "# Print the number of images in each category for the validation set\n",
    "print('Validation set:')\n",
    "for class_name in os.listdir(val_dir):\n",
    "    class_dir = os.path.join(val_dir, class_name)\n",
    "    print(class_name + ':', len(os.listdir(class_dir)))\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Create an instance of the VGG16 model\n",
    "vgg = models.vgg16(pretrained=True)\n",
    "\n",
    "# Create a new model that includes the VGG16 model and additional layers\n",
    "model = nn.Sequential(\n",
    "    vgg.features,\n",
    "    nn.Flatten(),yy\n",
    "    nn.Linear(25088, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and scheduler\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Create transforms for the training and validation data\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets for the training and validation data\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=val_transforms)\n",
    "test_dataset = ImageFolder(test_dir, transform=test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create data loaders for the training and validation data\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Lists to store the training and validation accuracy and loss\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        predicted = outputs > 0.5\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.float().unsqueeze(1)).sum().item()\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            predicted = outputs > 0.5\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.float().unsqueeze(1)).sum().item()\n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    val_acc = correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} -- Training Loss: {train_loss:.4f} -- Training Accuracy: {train_acc:.4f} -- Validation Loss: {val_loss:.4f} -- Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(range(1, epochs+1), train_accs, label='Training Accuracy')\n",
    "plt.plot(range(1, epochs+1), val_accs, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure()\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "true_labels = []  # List to store the true labels\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        predicted = outputs > 0.5\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.float().unsqueeze(1)).sum().item()\n",
    "        true_labels.extend(labels.cpu().numpy().flatten())  # Collect the true labels\n",
    "\n",
    "test_loss = test_loss / len(test_dataset)\n",
    "test_acc = correct / total\n",
    "print(f'Test Loss: {test_loss:.4f} -- Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Get the predicted labels for the test set\n",
    "predicted_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predicted = outputs > 0.5\n",
    "        predicted_labels.extend(predicted.cpu().numpy().flatten())\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix using Seaborn\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d86cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
